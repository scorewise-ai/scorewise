import os
import json
import asyncio
import aiofiles
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
import requests
from dotenv import load_dotenv
import PyPDF2
import io
import zipfile
import shutil
from fpdf import FPDF
import re
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

FONTS_DIR = os.path.join(os.path.dirname(__file__), "fonts")

class PDFReport(FPDF):
    def __init__(self):
        super().__init__()
        self.set_auto_page_break(auto=True, margin=15)
        # Register Unicode fonts BEFORE add_page()
        self.add_font('DejaVu', '', os.path.join(FONTS_DIR, 'DejaVuSans.ttf'), uni=True)
        self.add_font('DejaVu', 'B', os.path.join(FONTS_DIR, 'DejaVuSans-Bold.ttf'), uni=True)
        self.add_font('DejaVu', 'I', os.path.join(FONTS_DIR, 'DejaVuSans-Oblique.ttf'), uni=True)
        self.add_font('DejaVu', 'BI', os.path.join(FONTS_DIR, 'DejaVuSans-BoldOblique.ttf'), uni=True)
        self.add_page()

    def header(self):
        self.set_font('DejaVu', 'B', 16)
        self.cell(0, 10, 'ScoreWise AI Grading Report', 0, 1, 'C')
        self.ln(10)

    def chapter_title(self, title):
        self.set_font('DejaVu', 'B', 14)
        self.cell(0, 10, title, 0, 1, 'L')
        self.ln(2)

    def chapter_body(self, body):
        self.set_font('DejaVu', '', 11)
        self.multi_cell(0, 6, body)
        self.ln()

    def add_score_section(self, title, score, max_score=100):
        self.set_font('DejaVu', 'B', 12)
        self.cell(120, 8, title, 0, 0, 'L')
        self.cell(0, 8, f"{score}%", 0, 1, 'R')

class ScoreWiseGrader:
    def __init__(self):
        self.api_key = PERPLEXITY_API_KEY
        self.api_url = "https://api.perplexity.ai/chat/completions"
        self.default_rubrics = {
            "algebra": {
                "Problem Setup": {"weight": 0.25, "description": "Correctly identifies variables and sets up equations"},
                "Algebraic Manipulation": {"weight": 0.35, "description": "Uses proper algebraic techniques and operations"},
                "Solution Accuracy": {"weight": 0.25, "description": "Arrives at correct numerical answers"},
                "Work Presentation": {"weight": 0.15, "description": "Shows clear, organized work with proper notation"}
            },
            "biology": {
                "Conceptual Understanding": {"weight": 0.35, "description": "Demonstrates mastery of biological concepts and theories"},
                "Scientific Reasoning": {"weight": 0.30, "description": "Applies biological principles to analyze and solve problems"},
                "Data Interpretation": {"weight": 0.20, "description": "Correctly interprets biological data and experimental results"},
                "Scientific Communication": {"weight": 0.15, "description": "Uses proper biological terminology and clear explanations"}
            },
            "calculus": {
                "Mathematical Concepts": {"weight": 0.30, "description": "Understanding of calculus principles (limits, derivatives, integrals)"},
                "Problem-Solving Strategy": {"weight": 0.25, "description": "Selects appropriate calculus techniques and methods"},
                "Computational Accuracy": {"weight": 0.25, "description": "Performs calculations correctly with proper steps"},
                "Mathematical Communication": {"weight": 0.20, "description": "Clear notation and logical progression of mathematical work"}
            },
            "chemistry": {
                "Conceptual Understanding": {"weight": 0.30, "description": "Mastery of chemical principles and theories"},
                "Problem Solving": {"weight": 0.30, "description": "Application of concepts to solve chemical problems"},
                "Quantitative Analysis": {"weight": 0.25, "description": "Accurate calculations and stoichiometry"},
                "Scientific Communication": {"weight": 0.15, "description": "Clear explanation of chemical reasoning"}
            },
            "engineering": {
                "Engineering Principles": {"weight": 0.30, "description": "Application of fundamental engineering concepts"},
                "Problem-Solving Approach": {"weight": 0.25, "description": "Systematic approach to engineering problems"},
                "Technical Analysis": {"weight": 0.25, "description": "Quantitative analysis and mathematical reasoning"},
                "Professional Communication": {"weight": 0.20, "description": "Clear technical documentation and presentation"}
            },
            "physics": {
                "Physics Concepts": {"weight": 0.30, "description": "Understanding of physical principles and laws"},
                "Mathematical Application": {"weight": 0.25, "description": "Correct use of equations and mathematical tools"},
                "Problem-Solving Strategy": {"weight": 0.25, "description": "Logical approach to physics problems"},
                "Units & Dimensional Analysis": {"weight": 0.20, "description": "Proper use of units and dimensional consistency"}
            }
        }

    def extract_student_name(self, file_path: str) -> str:
        try:
            filename = os.path.basename(file_path)
            name = filename.replace('.pdf', '').replace('submission_', '').replace('_', ' ')
            name = re.sub(r'^\d+\s*', '', name)
            return name.strip() if name.strip() else "Unknown Student"
        except:
            return "Unknown Student"

    async def extract_text_from_pdf(self, file_path: str) -> str:
        try:
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
            text_content = ""
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text and page_text.strip():
                        text_content += f"\n--- Page {page_num + 1} ---\n{page_text}"
                except Exception as page_error:
                    logger.warning(f"Could not extract text from page {page_num + 1}: {str(page_error)}")
            if text_content.strip():
                return text_content.strip()
            else:
                return f"PDF file processed: {os.path.basename(file_path)} ({len(content)} bytes)"
        except Exception as e:
            logger.error(f"Error extracting PDF text: {str(e)}")
            return f"Error reading PDF {os.path.basename(file_path)}: {str(e)}"

    async def generate_pdf_report(self, student_name: str, result: dict, rubric: dict, subject: str, output_path: str):
        try:
            pdf = PDFReport()
            # Student header
            pdf.set_font('DejaVu', 'B', 14)
            pdf.chapter_title(f"Student: {student_name}")
            pdf.chapter_title(f"Subject: {subject.title()}")
            pdf.chapter_title(f"Date: {datetime.now().strftime('%B %d, %Y')}")
            pdf.ln(5)
            # Overall score section
            pdf.set_font('DejaVu', 'B', 16)
            pdf.set_fill_color(230, 230, 250)
            pdf.cell(0, 12, f"Overall Score: {result['overall_score']}%", 1, 1, 'C', 1)
            pdf.ln(5)
            # Grade interpretation
            score = result['overall_score']
            if score >= 90:
                grade_letter = "A"
                interpretation = "Excellent work!"
            elif score >= 80:
                grade_letter = "B"
                interpretation = "Good work!"
            elif score >= 70:
                grade_letter = "C"
                interpretation = "Satisfactory work."
            elif score >= 60:
                grade_letter = "D"
                interpretation = "Below expectations."
            else:
                grade_letter = "F"
                interpretation = "Needs significant improvement."
            pdf.set_font('DejaVu', 'B', 14)
            pdf.chapter_title(f"Letter Grade: {grade_letter} - {interpretation}")
            pdf.ln(5)
            # Rubric breakdown
            pdf.set_font('DejaVu', 'B', 14)
            pdf.chapter_title("Detailed Score Breakdown:")
            for criterion, score_val in result['rubric_scores'].items():
                weight = rubric[criterion]['weight']
                description = rubric[criterion]['description']
                pdf.set_font('DejaVu', 'B', 12)
                pdf.add_score_section(f"{criterion} ({weight*100:.0f}%)", score_val)
                pdf.set_font('DejaVu', 'I', 10)
                pdf.cell(0, 5, f"   {description}", 0, 1, 'L')
                pdf.ln(2)
            pdf.ln(5)
            # Feedback sections
            if result.get('feedback'):
                pdf.set_font('DejaVu', 'B', 14)
                pdf.chapter_title("Overall Feedback:")
                pdf.set_font('DejaVu', '', 11)
                pdf.chapter_body(result['feedback'])
            if result.get('strengths'):
                pdf.set_font('DejaVu', 'B', 14)
                pdf.chapter_title("Strengths Identified:")
                pdf.set_font('DejaVu', '', 11)
                for strength in result['strengths']:
                    pdf.chapter_body(f"• {strength}")
            if result.get('areas_for_improvement'):
                pdf.set_font('DejaVu', 'B', 14)
                pdf.chapter_title("Areas for Improvement:")
                pdf.set_font('DejaVu', '', 11)
                for improvement in result['areas_for_improvement']:
                    pdf.chapter_body(f"• {improvement}")
            if result.get('detailed_feedback'):
                pdf.set_font('DejaVu', 'B', 14)
                pdf.chapter_title("Detailed Analysis:")
                pdf.set_font('DejaVu', '', 11)
                pdf.chapter_body(result['detailed_feedback'])
            # Footer
            pdf.ln(10)
            pdf.set_font('DejaVu', 'I', 9)
            pdf.cell(0, 5, "Generated by ScoreWise AI - AI-Powered STEM Assignment Grading", 0, 1, 'C')
            # Save report
            pdf.output(output_path)
            logger.info(f"✓ PDF report generated: {output_path}")
        except Exception as e:
            logger.error(f"Error generating PDF report: {str(e)}")
            print(f"✗ Error generating PDF report: {str(e)}")
            raise

    async def create_reports_zip(self, task_dir: Path) -> str:
        try:
            reports_dir = task_dir / "reports"
            if not reports_dir.exists():
                logger.warning(f"No reports directory found at {reports_dir}")
                return ""
            zip_path = task_dir / "all_reports.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for report_file in reports_dir.glob("*.pdf"):
                    zipf.write(report_file, report_file.name)
            logger.info(f"✓ Created reports ZIP: {zip_path}")
            return str(zip_path)
        except Exception as e:
            logger.error(f"Error creating reports ZIP: {str(e)}")
            return ""

    async def grade_assignment(self, task_data: Dict) -> Dict:
        try:
            task_id = task_data["task_id"]
            subject = task_data["subject"]
            files = task_data["files"]
            logger.info(f"🎯 Starting grading for task {task_id}")
            task_dir = Path(f"uploads/{task_id}")
            reports_dir = task_dir / "reports"
            reports_dir.mkdir(exist_ok=True)
            assignment_text = ""
            if "assignment" in files:
                assignment_text = await self.extract_text_from_pdf(files["assignment"])
            solution_text = ""
            if "solution" in files:
                solution_text = await self.extract_text_from_pdf(files["solution"])
            rubric = self.default_rubrics.get(subject)
            if not rubric:
                rubric = {
                    "Problem Understanding": {"weight": 0.30, "description": "Demonstrates understanding of the problem"},
                    "Technical Knowledge": {"weight": 0.30, "description": "Shows knowledge of relevant concepts"},
                    "Solution Method": {"weight": 0.25, "description": "Uses appropriate methods and techniques"},
                    "Presentation": {"weight": 0.15, "description": "Clear and organized presentation of work"}
                }
            submission_results = []
            submissions = files.get("submissions", [])
            for i, submission_path in enumerate(submissions):
                student_name = self.extract_student_name(submission_path)
                submission_text = await self.extract_text_from_pdf(submission_path)
                individual_result = await self.grade_individual_submission(
                    assignment_text=assignment_text,
                    submission_text=submission_text,
                    solution_text=solution_text,
                    rubric=rubric,
                    subject=subject
                )
                individual_result["submission_id"] = i + 1
                individual_result["file_path"] = submission_path
                individual_result["student_name"] = student_name
                submission_results.append(individual_result)
                report_filename = f"{student_name.replace(' ', '_')}_report.pdf"
                report_path = reports_dir / report_filename
                await self.generate_pdf_report(
                    student_name=student_name,
                    result=individual_result,
                    rubric=rubric,
                    subject=subject,
                    output_path=str(report_path)
                )
            zip_path = await self.create_reports_zip(task_dir)
            overall_stats = self.calculate_overall_statistics(submission_results)
            results = {
                "task_id": task_id,
                "subject": subject,
                "rubric_used": rubric,
                "submission_count": len(submissions),
                "individual_results": submission_results,
                "overall_statistics": overall_stats,
                "reports_zip_path": zip_path,
                "processed_at": datetime.now().isoformat(),
                "status": "completed"
            }
            logger.info(f"🎉 Grading completed for {task_id}")
            return results
        except Exception as e:
            logger.error(f"✗ Grading error for {task_data.get('task_id', 'unknown')}: {str(e)}")
            return {
                "task_id": task_data.get("task_id", "unknown"),
                "status": "error",
                "error": str(e),
                "processed_at": datetime.now().isoformat()
            }

    async def grade_individual_submission(self, assignment_text: str, submission_text: str,
                                         solution_text: str, rubric: Dict, subject: str) -> Dict:
        prompt = self.create_grading_prompt(
            assignment_text, submission_text, solution_text, rubric, subject
        )
        try:
            response = await self.call_perplexity_api(prompt)
            ai_feedback = self.parse_ai_response(response)
            rubric_scores = {}
            total_weighted_score = 0
            for criterion, details in rubric.items():
                score = ai_feedback.get("scores", {}).get(criterion, 75)
                rubric_scores[criterion] = score
                total_weighted_score += score * details["weight"]
            result = {
                "overall_score": round(total_weighted_score),
                "rubric_scores": rubric_scores,
                "feedback": ai_feedback.get("feedback", "Good work overall."),
                "detailed_feedback": ai_feedback.get("detailed_feedback", ""),
                "strengths": ai_feedback.get("strengths", []),
                "areas_for_improvement": ai_feedback.get("improvements", []),
                "ai_confidence": ai_feedback.get("confidence", 0.8)
            }
            logger.info(f"✓ AI grading completed: {result['overall_score']}%")
            return result
        except Exception as e:
            logger.warning(f"⚠️ AI grading failed, using fallback: {str(e)}")
            return {
                "overall_score": 75,
                "rubric_scores": {k: 75 for k in rubric.keys()},
                "feedback": f"Automated grading completed. Manual review recommended. (AI Error: {str(e)[:100]})",
                "detailed_feedback": "The submission has been processed with basic scoring due to AI service issues.",
                "strengths": ["Submission received and processed"],
                "areas_for_improvement": ["Manual review recommended for detailed feedback"],
                "ai_confidence": 0.5
            }

    def create_grading_prompt(self, assignment_text: str, submission_text: str,
                             solution_text: str, rubric: Dict, subject: str) -> str:
        rubric_text = "\n".join([
            f"- {criterion} ({details['weight']*100:.0f}%): {details['description']}"
            for criterion, details in rubric.items()
        ])
        prompt = f"""
You are an expert {subject} educator grading a college-level STEM assignment. Please provide detailed, constructive feedback and accurate scoring.

ASSIGNMENT INSTRUCTIONS:
{assignment_text[:2000]}

GRADING RUBRIC:
{rubric_text}

STUDENT SUBMISSION:
{submission_text[:3000]}

{f"SOLUTION/ANSWER KEY: {solution_text[:1000]}" if solution_text else ""}

Please provide:
1. A score (0-100) for each rubric criterion
2. Overall constructive feedback
3. Specific strengths identified  
4. Areas for improvement
5. Detailed comments on the work

Format your response as JSON with the following structure:
{{
    "scores": {{
        "criterion_name": score_number,
        ...
    }},
    "feedback": "Overall feedback summary",
    "detailed_feedback": "Detailed analysis of the work",
    "strengths": ["strength 1", "strength 2", ...],
    "improvements": ["improvement 1", "improvement 2", ...],
    "confidence": 0.0-1.0
}}

Be fair, constructive, and specific in your feedback. Focus on helping the student learn and improve in {subject}.
"""
        return prompt

    async def call_perplexity_api(self, prompt: str) -> Dict:
        if not self.api_key:
            raise Exception("Perplexity API key not configured")
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "sonar-pro",
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert STEM educator providing fair, detailed, and constructive feedback on student work."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: requests.post(self.api_url, headers=headers, json=data, timeout=60)
        )
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"API error: {response.status_code} - {response.text}")

    def parse_ai_response(self, response: Dict) -> Dict:
        try:
            content = response["choices"][0]["message"]["content"]
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {
                    "scores": {},
                    "feedback": content[:500],
                    "detailed_feedback": content,
                    "strengths": [],
                    "improvements": [],
                    "confidence": 0.7
                }
        except Exception:
            return {
                "scores": {},
                "feedback": "AI grading completed with basic feedback.",
                "detailed_feedback": "The submission has been reviewed.",
                "strengths": ["Submission completed"],
                "improvements": ["Continue practicing"],
                "confidence": 0.6
            }

    def calculate_overall_statistics(self, submission_results: List[Dict]) -> Dict:
        if not submission_results:
            return {}
        scores = [result["overall_score"] for result in submission_results]
        return {
            "average_score": round(sum(scores) / len(scores), 1),
            "highest_score": max(scores),
            "lowest_score": min(scores),
            "total_submissions": len(submission_results),
            "grade_distribution": self.calculate_grade_distribution(scores)
        }

    def calculate_grade_distribution(self, scores: List[int]) -> Dict:
        distribution = {"A": 0, "B": 0, "C": 0, "D": 0, "F": 0}
        for score in scores:
            if score >= 90:
                distribution["A"] += 1
            elif score >= 80:
                distribution["B"] += 1
            elif score >= 70:
                distribution["C"] += 1
            elif score >= 60:
                distribution["D"] += 1
            else:
                distribution["F"] += 1
        return distribution

# Initialize grader instance
grader = ScoreWiseGrader()
